{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial set-up\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "from timeit import default_timer\n",
    "from utilities3 import *\n",
    "\n",
    "from Adam import Adam\n",
    "from sewar.full_ref import rmse, uqi\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CONFIGURATIONS - HYPER PARAMETERS\n",
    "################################################################\n",
    "#  configurations - HYPER PARAMETERS\n",
    "################################################################\n",
    "\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 100\n",
    "step_size = 50\n",
    "gamma = 0.5\n",
    "\n",
    "modes = 5\n",
    "width = 64\n",
    "\n",
    "weight_decay=1e-4\n",
    "\n",
    "# Hyperparameters to change - epoch{10,100,200}, step_size{}, batchsize{}, gamma{}, modes{}, weight_decay{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   DEFINING THE TOP 2D FOURIER LAYER\n",
    "#####################################################################\n",
    "#   DEFINING THE TOP 2D FOURIER LAYER\n",
    "\n",
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        #print('input',input.shape)\n",
    "        #print('weights',weights.shape)\n",
    "        #print(\"bixy,ioxy->boxy\")\n",
    "        R_out = torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "        #print(R_out.shape)\n",
    "        return R_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        #print('what is out_ft', out_ft.shape)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNO2d(nn.Module):\n",
    "    def __init__(self, modes1, modes2,  width):\n",
    "        super(FNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.padding = 16 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(3, self.width) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        #print('x grid input shape', x.shape, 'grid input shape', grid.shape)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        #x = F.pad(x, [0,self.padding, 0,self.padding])\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        #x = x[..., :-self.padding, :-self.padding]\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        #print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        #print(x.shape)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "################################ 1 FOURIER LAYER NETWORK ARCHITECTURE ###########################################\n",
    "\n",
    "class FNO2d_1L(nn.Module):\n",
    "    def __init__(self, modes1, modes2,  width):\n",
    "        super(FNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.padding = 16 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(3, self.width) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        #print('x grid input shape', x.shape, 'grid input shape', grid.shape)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        print(x.shape)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        #x = F.pad(x, [0,self.padding, 0,self.padding])\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        #x = x[..., :-self.padding, :-self.padding]\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "############################ 2 FOURIER LAYER NETWORK ARCHITECTURE #############################################\n",
    "class FNO2d_2L(nn.Module):\n",
    "    def __init__(self, modes1, modes2,  width):\n",
    "        super(FNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.padding = 16 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(3, self.width) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        #print('x grid input shape', x.shape, 'grid input shape', grid.shape)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        #x = F.pad(x, [0,self.padding, 0,self.padding])\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "\n",
    "        #x = x[..., :-self.padding, :-self.padding]\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################ 4 FOURIER LAYER NETWORK ARCHITECTURE #############################################\n",
    "class FNO2d_4L(nn.Module):\n",
    "    def __init__(self, modes1, modes2,  width):\n",
    "        super(FNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.padding = 16 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(3, self.width) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        #print('x grid input shape', x.shape, 'grid input shape', grid.shape)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        #x = F.pad(x, [0,self.padding, 0,self.padding])\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        #x = x[..., :-self.padding, :-self.padding]\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).to(device)\n",
    "\n",
    "\n",
    "############################ 6 FOURIER LAYER NETWORK ARCHITECTURE #############################################\n",
    "class FNO2d_6L(nn.Module):\n",
    "    def __init__(self, modes1, modes2,  width):\n",
    "        super(FNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.padding = 16 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(3, self.width) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv4 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv5 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w4 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w5 = nn.Conv2d(self.width, self.width, 1)\n",
    "\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        #print('x grid input shape', x.shape, 'grid input shape', grid.shape)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        #x = F.pad(x, [0,self.padding, 0,self.padding])\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        #x = x[..., :-self.padding, :-self.padding]\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = scipy.io.loadmat('./datasets/10k_real_dataset')\n",
    "\n",
    "sol = np.asarray(raw_data['solution_data'])\n",
    "mes = np.asarray(raw_data['measurement_data'])\n",
    "\n",
    "sol_temp = np.expand_dims(sol,axis = 1 )\n",
    "#sol_temp = np.sqeeze(sol_temp)\n",
    "mes_temp = np.transpose(np.expand_dims(mes,axis = 1 ),(0,2,1))\n",
    "\n",
    "# dimensions of solution space\n",
    "y_dim1 = 16\n",
    "y_dim2 = 16\n",
    "\n",
    "# dimensions of measurement space\n",
    "x_dim2 = 8     # no of detectors\n",
    "x_dim1 = int(mes.shape[1]/x_dim2)\n",
    "\n",
    "def convert_data(data_x, data_y):\n",
    "    data_X = torch.from_numpy(data_x).float()\n",
    "    data_Y = torch.from_numpy(data_y).float()\n",
    "    return data_X, data_Y\n",
    "\n",
    "# we are solving the inverse problem, so going from measurements, to solutions\n",
    "X, y = convert_data(mes_temp, sol_temp)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#y_normalizer = UnitGaussianNormalizer(y_train)\n",
    "#y_train = y_normalizer.encode(y_train)\n",
    "\n",
    "#x_normalizer = UnitGaussianNormalizer(X_train)\n",
    "#X_train = x_normalizer.encode(X_train)\n",
    "#X_test = x_normalizer.encode(X_test)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "y_test = y_test.reshape(y_test.shape[0],y_dim1,y_dim2,1)\n",
    "y_train = y_train.reshape(y_train.shape[0],y_dim1,y_dim2,1)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], x_dim1,x_dim2,1)\n",
    "X_train = X_train.reshape(X_train.shape[0], x_dim1,x_dim2,1)\n",
    "\n",
    "#batch_size = 10\n",
    "#train_loader = DataLoader(data_utils.TensorDataset(X_train, Y_train), batch_size=batch_size, shuffle=True)\n",
    "train_dataset = TensorDataset( X_train, y_train )\n",
    "test_dataset = TensorDataset( X_test, y_test )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8000, 16, 16, 1]), torch.Size([8000, 8, 8, 1]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = X_train.shape[0]\n",
    "ntest = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL METRICS\n",
    "###################################\n",
    "# MODEL METRICS\n",
    "###################################\n",
    "def model_metrics(model,test_loader,ntrain):\n",
    "    rms_values, psnr_values, ssim_values, uqi_values = [], [], [], []\n",
    "\n",
    "    for test_num in range(ntest):\n",
    "        K = torch.unsqueeze(test_loader.dataset[test_num][0], 0).cuda()\n",
    "        model.eval()\n",
    "        predicted_np = np.reshape(model(K).detach().cpu().numpy(),(16,16))\n",
    "\n",
    "        truth = test_loader.dataset[test_num][1]\n",
    "        truth_np = np.reshape(truth.numpy(),(16,16))\n",
    "\n",
    "        #diff_image = predicted - truth_np\n",
    "        #np.sqrt(np.sum(diff_image**2)/256)\n",
    "\n",
    "        rms_values.append(rmse(predicted_np, truth_np))\n",
    "        psnr_values.append(psnr(truth_np, predicted_np, data_range=predicted_np.max() - predicted_np.min()))\n",
    "        ssim_values.append(ssim(truth_np, predicted_np, data_range=predicted_np.max() - predicted_np.min()))\n",
    "        uqi_values.append(uqi(predicted_np, truth_np))\n",
    "\n",
    "    model_rms = sum(rms_values)/ len(rms_values)\n",
    "    std_rms = np.std(np.array(rms_values))\n",
    "\n",
    "    model_psnr = sum(psnr_values)/ len(psnr_values)\n",
    "    std_psnr = np.std(np.array(psnr_values))\n",
    "\n",
    "    model_ssim = sum(ssim_values)/ len(ssim_values)\n",
    "    std_ssim = np.std(np.array(ssim_values))\n",
    "\n",
    "    model_uqi = sum(uqi_values)/ len(uqi_values)\n",
    "    std_uqi = np.std(np.array(uqi_values))\n",
    "\n",
    "\n",
    "    print(\"RMSE: \", model_rms, std_rms, sep=\"---\")\n",
    "    print(\"PSNR: \", model_psnr, std_psnr, sep=\"---\")\n",
    "    print(\"SSIM: \", model_ssim, std_ssim, sep=\"---\")\n",
    "    print(\"UQI: \", model_uqi, std_uqi, sep=\"---\")\n",
    "\n",
    "\n",
    "    output = {\n",
    "    \"rms\": rms_values,\n",
    "    \"psnr\": psnr_values,\n",
    "    \"ssim\": ssim_values,\n",
    "    \"uqi\": uqi_values,\n",
    "    }\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND EVALUATION\n",
    "################################################################\n",
    "# TRAINING AND EVALUATION\n",
    "################################################################\n",
    "\n",
    "\n",
    "def train_model(model, epochs, batch_size, learning_rate, weight_decay, step_size, gamma):\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    train_losses, test_losses, epoch_time = [], [], []\n",
    "\n",
    "    myloss = LpLoss(size_average=False)\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        t1 = default_timer()\n",
    "        train_mse = 0\n",
    "        train_l2 = 0\n",
    "        total_time = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            #print('in: ',x.shape)\n",
    "            out = model(x)\n",
    "            #print('out: ',out.shape)\n",
    "\n",
    "            mse = F.mse_loss(out.view(batch_size, -1), y.view(batch_size, -1), reduction='mean')\n",
    "            l2 = myloss(out.view(batch_size, -1), y.view(batch_size, -1))\n",
    "            l2.backward() # use the l2 relative loss\n",
    "\n",
    "            optimizer.step()\n",
    "            train_mse += mse.item()\n",
    "            train_l2 += l2.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        model.eval()\n",
    "        test_l2 = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "\n",
    "                out = model(x)\n",
    "                test_l2 += myloss(out.view(batch_size, -1), y.view(batch_size, -1)).item()\n",
    "\n",
    "        #train_mse /= len(train_loader)\n",
    "        train_l2 /= ntrain\n",
    "        train_losses.append(train_l2)\n",
    "        test_l2 /= ntest\n",
    "        test_losses.append(test_l2)\n",
    "\n",
    "\n",
    "        t2 = default_timer()\n",
    "        epoch_time.append(t2 - t1)\n",
    "\n",
    "        print(ep, t2 - t1, train_l2, test_l2)\n",
    "    total_time = sum(epoch_time)\n",
    "    print(\"TOTAL TIME: \", total_time, sep=\"---\")\n",
    "\n",
    "    output = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"epoch_time\": epoch_time,\n",
    "    \"training_time\": total_time,\n",
    "    \"model\": model\n",
    "    }\n",
    "    return output \n",
    "# torch.save(model, 'model/ns_fourier_burgers')\n",
    "\n",
    "# scipy.io.savemat('pred/burger_test.mat', mdict={'pred': pred.cpu().numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664132\n",
      "0 12.034972900059074 0.15676114691048862 0.12655275729298593\n",
      "1 11.510819899966009 0.11751570723950863 0.11493624573945999\n",
      "2 11.71083770005498 0.11170549504458904 0.10882448366284371\n",
      "3 12.497813599999063 0.10818859228491783 0.10527719867229461\n",
      "4 12.393504700041376 0.10443783429265022 0.10642559963464737\n",
      "5 11.819936799933203 0.10153866245597601 0.10714973333477974\n",
      "6 11.837564499932341 0.10011104295402765 0.09624152961373329\n",
      "7 12.221078000031412 0.0968039288893342 0.0948927254974842\n",
      "8 11.8407501000911 0.09619913090020418 0.09728874477744102\n",
      "9 12.143095000064932 0.09384076257050038 0.09178387424349785\n",
      "10 11.88642350002192 0.0925491386577487 0.09006020727753639\n",
      "11 11.830368099967018 0.09090042489767075 0.09008493122458458\n",
      "12 11.773758299998008 0.08911076858639717 0.08990910214185714\n",
      "13 11.763125999947079 0.08797497218102217 0.09080900621414184\n",
      "14 11.88366210006643 0.08662761784344912 0.08941302111744881\n",
      "15 11.948627199977636 0.08562286544591188 0.08893635100126267\n",
      "16 12.268423300003633 0.08447390474379063 0.08461067336797715\n",
      "17 11.784632400027476 0.08348376607149839 0.08281820616126061\n",
      "18 11.795351700042374 0.08249298952519894 0.08331465655565262\n",
      "19 11.79551900003571 0.0817431070432067 0.0837753711938858\n",
      "TOTAL TIME: ---238.74026480026077\n",
      "RMSE: ---0.0022681500307549047---0.001061721119600402\n",
      "PSNR: ---27.85976503326506---2.817438282723945\n",
      "SSIM: ---0.9224979864222356---0.023573798812431594\n",
      "UQI: ---0.9960375885219536---0.0029366701723930956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1be8dfd8310>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD4CAYAAAAjDTByAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAARTElEQVR4nO3de4xd1XXH8e/vzsM2xhiMCS+T8BABkSfIRUBaSkuJCEV2kKoKBA1tkFCk0EDVKCGiavJX1SRt+kqUiARS2lCQGqBByLS4BISq1ijg2mBiiA04YGwwj8TGNp7Hvat/3EM7HmaY2fueezzu/n2k0dzHWbPX7HPXPfe171JEYGblaR3oBMzswHDxmxXKxW9WKBe/WaFc/GaFGmxysOHW/FjQWpQeONffkcjNT8ocLn08ZY7V5P8WnU7eWBnUyjzu5c4j6fMYnfSYfbGH0dg3qyQbLf4FrUWcu/iy5LgYH08frMEbEu12XtzQUFZYjI4mx2hgIGus7HnM+N86e/bmjZWhNX9eVpzm5cXl3EY6IyPJMWtG7p/1tn7Yb1YoF79ZoXoqfkkXS3pG0mZJN9aVlJn1X3bxSxoAvgV8AjgDuELSGXUlZmb91cuR/2xgc0Q8FxGjwJ3AynrSMrN+66X4jwdenHB+a3XZfiRdK+kxSY+Nxr4ehjOzOvVS/FO9l/iONyYj4uaIWB4Ry4c1v4fhzKxOvRT/VuCECeeXAdt6S8fMmtJL8f8EOFXSSZKGgcuBe+tJy8z6LfsTfhExLuk64N+AAeDWiHiqtszMrK96+nhvRKwCVtWUi5k1yJ/wMytUowt7iGhukU6Di202fffUrKHa+5qbfrXyVudpMG9hT7TTV7/l5pjjtM8+29hYc5WP/GaFcvGbFcrFb1YoF79ZoVz8ZoVy8ZsVysVvVigXv1mhXPxmhXLxmxXKxW9WKBe/WaGaXdijvLZRWcs9MtsxnfLj9G44O17O6zTzxkhG6zKglbHYJiKvzVRudyoNpu+11kDeYqwF88eSY168bVnWWMuu3JIVp0MWpMdkLE7T6Ox3mI/8ZoVy8ZsVysVvVqheOvacIOkhSRslPSXp+joTM7P+6uUFv3HgjyNiraRFwOOSVkfET2vKzcz6KPvIHxHbI2JtdfpNYCNTdOwxs7mplrf6JJ0InAk8OsV11wLXAszXwjqGM7Ma9PyCn6RDgbuAGyJi1+Tr92vX1XK7LrO5oqfilzREt/Bvj4i760nJzJrQy6v9Am4BNkbEN+pLycya0MuR/2PA7wG/KWld9XNJTXmZWZ/10qvvP5i6TbeZHQT8CT+zQjW7qg9BK+PBQqTfR53x8J70cYCPLHwhOWYsIz+Ah3a/PyuuUcproTUwkB63ZFHePutkrFhsd/L22S/vOjYr7oirfpEck9PaLmL28+4jv1mhXPxmhXLxmxXKxW9WKBe/WaFc/GaFcvGbFcrFb1YoF79ZoVz8ZoVy8ZsVysVvVqhGF/ZEu0175zu+6WtGu1adnBxz3qLVyTEAv7Zge3LM+j3vzRqrNZDedgugPT6QHKPMBTqtVu7CnvT/baiVNx97x4aSY8Yy5hBg10jeV9G9/s1Dk2NO/tTGrLFmy0d+s0K5+M0K5eI3K1QdX909IOm/Jd1XR0Jm1ow6jvzX0+3WY2YHkV6/t38Z8NvA9+pJx8ya0uuR/6+BLwB579GY2QHTS9OOS4EdEfH4DNtdK+kxSY+NMZI7nJnVrNemHSskbQHupNu84weTN5rYq2+IeT0MZ2Z16qVF95ciYllEnAhcDvw4Iq6qLTMz6yu/z29WqFo+2x8RDwMP1/G3zKwZPvKbFarZdl0SGkxfgfXhI7clx4xF3qqt58bSV23tbue9kNlp5933jo+k/2+twdwVhHk5ju5Lb6G1dV/6bQNg3ryx5JjIaPEFMDbSYMlEzj5zuy4zm4GL36xQLn6zQrn4zQrl4jcrlIvfrFAufrNCufjNCuXiNyuUi9+sUC5+s0K5+M0K5eI3K1Szq/oiiLHR5LAVS55Ojnl57PDkGIBdnfRVfW+181ajjY/mrTxkNP0+OzJ77rWG8lYDdkbSc+x08uZjJGOFXuRNB53cfdbJWEWonGPz7Mfxkd+sUC5+s0K5+M0K1WvHnsMl/VDS05I2Sjq3rsTMrL96fcHvb4B/jYjfkTQMHFJDTmbWgOzil3QYcD7w+wARMQqkv5RvZgdELw/7TwZeBb5ftej+nqSFkzdyuy6zuamX4h8EzgK+HRFnAnuAGydv5HZdZnNTL8W/FdgaEY9W539I987AzA4CvfTqexl4UdJp1UUXAj+tJSsz67teX+3/Q+D26pX+54A/6D0lM2tCT8UfEeuA5fWkYmZNanZhD0ArfWHEM/uOS45ZMrg7OQZgfiu99dMph7yWNdZ/Dp6SFdfOWCQyNH88a6zRvXmLlvIWH+UNlbP0KHfBUraM4WI8/baYsmLJH+81K5SL36xQLn6zQrn4zQrl4jcrlIvfrFAufrNCufjNCuXiNyuUi9+sUC5+s0K5+M0K5eI3K1Tzq/o67eSQZ/YenRzzu0f+PDkG4ENDe5NjXpz/StZYQ8N5K+3arfTdltueShldprrjZQRmJhnt9LE6uf9YxmpFAGWsxNRgxorKMbfrMrMZuPjNCuXiNytUr+26/kjSU5I2SLpDUnp/azM7ILKLX9LxwOeA5RHxQWAAuLyuxMysv3p92D8ILJA0SLdP37beUzKzJvTyvf0vAX8BvABsB3ZGxAOTt3O7LrO5qZeH/UcAK4GTgOOAhZKumryd23WZzU29POz/LeD5iHg1IsaAu4Hz6knLzPqtl+J/AThH0iGSRLdd18Z60jKzfuvlOf+jdJtzrgWerP7WzTXlZWZ91mu7ri8DX64pFzNrkD/hZ1aoZlf1SWhoODls866jkmO2LEqPAThhcFdWXI6xsfS+hQAMpq9+a2eOFWOZx4eBjBV6OTEAGav6WhlzCNDOzDFa6XEaSi9PjXtVn5nNwMVvVigXv1mhXPxmhXLxmxXKxW9WKBe/WaFc/GaFcvGbFcrFb1YoF79ZoVz8ZoVqdGGPJDSc3oLo+ZeWJsf8E2cnxwAcc+Ivk2Me2fn+rLGGM9t1vbU7fQ47uYuIchfbdDJilDeWhjLiMsdqLcjbZ52RjPnvZExiQsszH/nNCuXiNyuUi9+sUDMWv6RbJe2QtGHCZUskrZa0qfp9RH/TNLO6zebI//fAxZMuuxF4MCJOBR6szpvZQWTG4o+IR4A3Jl28EritOn0b8Ml60zKzfst9zn90RGwHqH6/Z7oNJ7brGo19mcOZWd36/oLfxHZdw+7gbTZn5Bb/K5KOBah+76gvJTNrQm7x3wtcXZ2+GvhRPemYWVNm81bfHcB/AadJ2irpGuDPgYskbQIuqs6b2UFkxs/2R8QV01x1Yc25mFmD/Ak/s0I1365rOL1dF5Hejun5bekrAQH+bPyS5JjXdh6aNdbornlZcaRPR+MGFrSTYxYd+lbWWJ2M28eePXnvPGWvjuyk59gZGUmOCa/qM7OZuPjNCuXiNyuUi9+sUC5+s0K5+M0K5eI3K5SL36xQLn6zQrn4zQrl4jcrlIvfrFANL+wBWukLHE7/3KbkmI1fOz05BvIW6XTaefehAwvHsuJyFjq19+Tt6uPe+3pW3G8ck77PLlv8eNZYr7YXJcfc94uPZo3178/ntWY76Zot6UGHpt8WtXv2t0Uf+c0K5eI3K5SL36xQue26vi7paUlPSLpH0uF9zdLMapfbrms18MGI+DDwM+BLNedlZn2W1a4rIh6IiPHq7BpgWR9yM7M+quM5/6eB+6e7cr92XR236zKbK3oqfkk3AePA7dNts1+7rpbbdZnNFdkf8pF0NXApcGGkfGWomc0JWcUv6WLgi8CvR8TeelMysybktuv6JrAIWC1pnaTv9DlPM6tZbruuW/qQi5k1yJ/wMytUs6v6OkG81czbfaff8ERW3LPfPy05ppPRigmgszdz+gfTX19tLRifeaMprDj+yay4yw5blxzzvsGMVm7AhtE3k2NWHLE2a6xNv5LeQgtAhx2WHpSxAhbNPsZHfrNCufjNCuXiNyuUi9+sUC5+s0K5+M0K5eI3K5SL36xQLn6zQrn4zQrl4jcrlIvfrFAufrNCNbuqD5JWHf2vVsZ9VKeTHgOcdOWGmTea5LkffChrrM5A3refaTD9f1u8OO8Ll1rKm8d2Rj/BeRrKGuuogd3JMX/ygQuyxhpYuiArLmc1a7yV3ssxEm73PvKbFcrFb1aorHZdE677vKSQtLQ/6ZlZv+S260LSCcBFwAs152RmDchq11X5K+ALgL+z3+wglPWcX9IK4KWIWD+Lbf+vXVe4XZfZXJH8Vp+kQ4CbgI/PZvuIuBm4GWDxwFI/SjCbI3KO/KcAJwHrJW2h26F3raRj6kzMzPor+cgfEU8C73n7fHUHsDwiXqsxLzPrs9x2XWZ2kMtt1zXx+hNry8bMGuNP+JkVShHNvQC/ePCoOPewlclx0c5YXDKWvigidywNZN6H5ixYAqLdTo8Zy2vX1RrOW2yT87/l5pgzHxrKW9PWmjcvKy7GM/63jMVpa/atYmfn9VmtqvKR36xQLn6zQrn4zQrl4jcrlIvfrFAufrNCufjNCuXiNyuUi9+sUC5+s0K5+M0K5eI3K5SL36xQja7qk/Qq8PNprl4KzIVvA3Ie+3Me+5vrebwvIo6azR9otPjfjaTHImK583AezqOZPPyw36xQLn6zQs2l4r/5QCdQcR77cx77+3+Tx5x5zm9mzZpLR34za5CL36xQjRa/pIslPSNps6Qbp7hekv62uv4JSWf1IYcTJD0kaaOkpyRdP8U2F0jaKWld9fOndecxYawtkp6sxnlsiuv7OieSTpvwf66TtEvSDZO26dt8SLpV0g5JGyZctkTSakmbqt9HTBP7rrenGvL4uqSnq3m/R9Lh08S+6z6sIY+vSHppwvxfMk1s2nxERCM/wADwLHAyMAysB86YtM0lwP2AgHOAR/uQx7HAWdXpRcDPpsjjAuC+huZlC7D0Xa7v+5xM2kcv0/2gSCPzAZwPnAVsmHDZ14Abq9M3Al/NuT3VkMfHgcHq9FenymM2+7CGPL4CfH4W+y5pPpo88p8NbI6I5yJiFLgTmPwl/iuBf4iuNcDhko6tM4mI2B4Ra6vTbwIbgePrHKNmfZ+TCS4Eno2I6T6FWbuIeAR4Y9LFK4HbqtO3AZ+cInQ2t6ee8oiIByLi7S/cX0O3KW1fTTMfs5E8H00W//HAixPOb+WdRTebbWoj6UTgTODRKa4+V9J6SfdL+kC/cgACeEDS45KuneL6JufkcuCOaa5raj4Ajo6I7dC9s2ZCY9gJGr2tAJ+m+whsKjPtwzpcVz39uHWap0HJ89Fk8U/VRWTy+4yz2aYWkg4F7gJuiIhdk65eS/eh70eAvwP+pR85VD4WEWcBnwA+K+n8yalOEVP7nEgaBlYA/zzF1U3Ox2w1eVu5CRgHbp9mk5n2Ya++DZwCfBTYDvzlVGlOcdm7zkeTxb8VOGHC+WXAtoxteiZpiG7h3x4Rd0++PiJ2RcTu6vQqYEjS0rrzqP7+tur3DuAeug/fJmpkTujecNdGxCtT5NjYfFReefupTfV7xxTbNHVbuRq4FLgyqifXk81iH/YkIl6JiHZEdIDvTvP3k+ejyeL/CXCqpJOqo8zlwL2TtrkX+FT1Cvc5wM63H/7VRZKAW4CNEfGNabY5ptoOSWfTnafX68yj+tsLJS16+zTdF5g2TNqs73NSuYJpHvI3NR8T3AtcXZ2+GvjRFNvM5vbUE0kXA18EVkTE3mm2mc0+7DWPia/xXDbN30+fjzpeoUx4JfMSuq+uPwvcVF32GeAz1WkB36qufxJY3occfpXuw6EngHXVzyWT8rgOeIruK6ZrgPP6NB8nV2Osr8Y7UHNyCN1iXjzhskbmg+4dznZgjO7R6xrgSOBBYFP1e0m17XHAqne7PdWcx2a6z6Pfvp18Z3Ie0+3DmvP4x2rfP0G3oI+tYz788V6zQvkTfmaFcvGbFcrFb1YoF79ZoVz8ZoVy8ZsVysVvVqj/AXQJNcTwL3WsAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ONE LAYER RESULTS\n",
    "############################################################################\n",
    "\n",
    "model_1L = FNO2d(modes, modes, width).cuda()\n",
    "print(count_params(model_1L))\n",
    "output_1L = train_model(model_1L, 20, batch_size, learning_rate, weight_decay, step_size, gamma)\n",
    "\n",
    "output_1L_model = output_1L.get(\"model\")\n",
    "#torch.save(output_1L_model, 'fno2D_1l.h5')\n",
    "results_1L = model_metrics(output_1L_model,test_loader,ntrain)\n",
    "\n",
    "# ONE LAYER RESULTS\n",
    "############################################################################\n",
    "test_num = 10\n",
    "K = torch.unsqueeze(test_loader.dataset[test_num][0], 0).cuda()\n",
    "output_1L_model.eval()\n",
    "predicted = np.reshape(output_1L_model(K).detach().cpu().numpy(),(16,16))\n",
    "plt.imshow(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\azhir\\Desktop\\final_results\\fno2_attempt.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model(x)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SpectralConv2d' object has no attribute 'weight1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\azhir\\Desktop\\final_results\\fno2_attempt.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(output_1L_model\u001b[39m.\u001b[39;49mconv1\u001b[39m.\u001b[39;49mweight1\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\Users\\azhir\\anaconda3\\envs\\gundam\\lib\\site-packages\\torch\\nn\\modules\\module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1184\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1185\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1186\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SpectralConv2d' object has no attribute 'weight1'"
     ]
    }
   ],
   "source": [
    "print(output_1L_model.conv1.weight1.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.type of SpectralConv2d()>\n"
     ]
    }
   ],
   "source": [
    "print(output_1L_model.conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_2L' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\azhir\\Desktop\\final_results\\fno2_attempt.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m fig1, ax1 \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m red_cross \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(markerfacecolor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, marker\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data \u001b[39m=\u001b[39m (results_1L\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrms\u001b[39m\u001b[39m\"\u001b[39m), results_2L\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrms\u001b[39m\u001b[39m\"\u001b[39m), results_4L\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrms\u001b[39m\u001b[39m\"\u001b[39m), results_6L\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrms\u001b[39m\u001b[39m\"\u001b[39m), results_8L\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrms\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m labels \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m1 fourier layer\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m2 fourier layers\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m4 fourier layers\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m6 fourier layers\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m8 fourier layers\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ax1\u001b[39m.\u001b[39mboxplot(data, vert\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, flierprops\u001b[39m=\u001b[39mred_cross)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results_2L' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "red_cross = dict(markerfacecolor='r', marker='x')\n",
    "data = (results_1L.get(\"rms\"), results_2L.get(\"rms\"), results_4L.get(\"rms\"), results_6L.get(\"rms\"), results_8L.get(\"rms\"))\n",
    "labels = ('1 fourier layer', '2 fourier layers', '4 fourier layers', '6 fourier layers', '8 fourier layers')\n",
    "\n",
    "ax1.boxplot(data, vert=False, flierprops=red_cross)\n",
    "plt.yticks(np.arange(len(labels))+1,labels)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE LAYER RESULTS\n",
    "############################################################################\n",
    "\n",
    "model_1L = FNO2d(modes, width).cuda()\n",
    "print(count_params(model_1L))\n",
    "output_1L = train_model(model_1L, 20, batch_size, learning_rate, weight_decay, step_size, gamma)\n",
    "\n",
    "output_1L_model = output_1L.get(\"model\")\n",
    "torch.save(output_1L_model, 'fno1_1l.h5')\n",
    "results_1L = model_metrics(output_1L_model,test_loader,ntrain)\n",
    "\n",
    "# ONE LAYER RESULTS\n",
    "############################################################################\n",
    "K = torch.unsqueeze(test_loader.dataset[test_num][0], 0).cuda()\n",
    "output_1L_model.eval()\n",
    "predicted = np.reshape(output_1L_model(K).detach().cpu().numpy(),(16,16))\n",
    "plt.imshow(predicted)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual\n",
    "test_num = 10\n",
    "truth_out = test_loader.dataset[test_num][1]\n",
    "plt.imshow(np.reshape(truth_out.numpy(),(16,16)))\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('gundam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cddc3d62f9a7e02ecb46a81ab11648d63c0573e5e13f25e036352996103f8463"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
