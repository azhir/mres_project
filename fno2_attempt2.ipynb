{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\azhir\\anaconda3\\envs\\gundam\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# initial set-up\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "from timeit import default_timer\n",
    "from utilities3 import *\n",
    "\n",
    "from Adam import Adam\n",
    "from sewar.full_ref import rmse, uqi\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# fourier layer\n",
    "################################################################\n",
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        print('bixy,ioxy->boxy: input is', input.shape, 'weights are', weights.shape)\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "        print('before FFT',x.shape,'after FFT',x_ft.shape)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNO2d(nn.Module):\n",
    "    def __init__(self, modes1, modes2,  width):\n",
    "        super(FNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.padding = 9 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(3, self.width) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        print('shape before concat', x.shape)\n",
    "        x = self.fc0(x)\n",
    "        print('shape after concat', x.shape)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        #x = F.pad(x, [0,self.padding, 0,self.padding])\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        #x = x[..., :-self.padding, :-self.padding]\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# configs\n",
    "################################################################\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 100\n",
    "step_size = 100\n",
    "gamma = 0.5\n",
    "\n",
    "modes = 12\n",
    "width = 32\n",
    "\n",
    "s = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 128]), torch.Size([10000, 256]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################\n",
    "# load data and data normalization\n",
    "################################################################\n",
    "reader = MatReader('./datasets/training_data_16x16_10k')\n",
    "y_raw = reader.read_field('solution_data')\n",
    "x_raw = reader.read_field('measurement_data')\n",
    "\n",
    "ndata = x_raw.shape[0]\n",
    "x_raw.shape, y_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_temp = x_raw.reshape(ndata,16,8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 16, 8, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f6bf8a1c90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD4CAYAAAAjDTByAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAM0ElEQVR4nO3de6xlZX3G8e9TLlIQC5Sq3CJgCA3VFsgE8RJqSlGkhLFJYyDVTqsJMSktNDU6hqT6Z62tvRoNFVpqCcQqVGKghVCNMSkTYQoMOCiXchkYGVoSsJpyqb/+sRfJ4XjOzDl7Xeaceb+fZLLX3uvde/3m3es5a+2119pvqgpJ7fmpvV2ApL3D8EuNMvxSowy/1CjDLzVq/ykXdmBeVQdxyJSLlJryv/yQF+r5rKTtpOE/iEN4S86ecpFSU7bUbStu626/1CjDLzWqV/iTnJvku0keTLJ5qKIkjW/u8CfZD/gs8B7gFOCiJKcMVZikcfXZ8p8BPFhVD1fVC8B1wMZhypI0tj7hPwZ4fMH9Hd1jr5Dk4iR3JLnjRZ7vsThJQ+oT/qW+S/yJSwSr6oqq2lBVGw7gVT0WJ2lIfcK/Azhuwf1jgSf7lSNpKn3C/23gpCQnJDkQuBC4cZiyJI1t7jP8quqlJJcA/wrsB1xVVfcNVpmkUfU6vbeqbgJuGqgWSRPyDD+pUZNe2LOvevRLb97bJWiV3vC+bXu7hL3OLb/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjvLBnES/SacO87/O+dEGQW36pUYZfapThlxrVZ8Se45J8Pcn2JPcluXTIwiSNq88Bv5eAP6yqrUkOBe5McmtVfWeg2iSNaO4tf1XtrKqt3fQPgO0sMWKPpLVpkK/6khwPnAZsWWLexcDFAAdx8BCLkzSA3gf8krwa+ApwWVU9t3i+w3VJa1Ov8Cc5gFnwr6mq64cpSdIU+hztD3AlsL2qPjNcSZKm0GfL/3bgA8CvJLmr+3feQHVJGlmfsfq+xdLDdEtaBzzDT2rUPntVn1fnaQz70tWAbvmlRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcatS4u7PEiHa1386zDY18M5JZfapThlxpl+KVGDfHT3fsl+Y8kXxuiIEnTGGLLfymz0XokrSN9f7f/WODXgC8MU46kqfTd8v8F8FHgx/1LkTSlPoN2nA/sqqo799Du4iR3JLnjRZ6fd3GSBtZ30I4LkjwCXMds8I5/XNzIsfqktanPEN0fr6pjq+p44ELg36rq/YNVJmlUfs8vNWqQc/ur6hvAN4Z4LUnTcMsvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKj+o7Yc1iSLye5P8n2JG8dqjBJ4+r7A55/CfxLVf1GkgOBgweoSdIE5g5/ktcAZwG/DVBVLwAvDFOWpLH12e0/EXga+LtuiO4vJDlkcSOH65LWpj7h3x84HfhcVZ0G/BDYvLiRw3VJa1Of8O8AdlTVlu7+l5n9MZC0DvQZq+/7wONJTu4eOhv4ziBVSRpd36P9vwdc0x3pfxj4nf4lSZpCr/BX1V3AhmFKkTQlz/CTGmX4pUYZfqlRhl9qlOGXGmX4pUYZfqlRhl9qlOGXGmX4pUYZfqlRhl9qlOGXGmX4pUYZfqlRhl9qlOGXGmX4pUb1Ha7rD5Lcl+TeJNcmOWiowiSNa+7wJzkG+H1gQ1W9CdgPuHCowiSNq+9u//7ATyfZn9k4fU/2L0nSFPr8bv8TwJ8CjwE7gWer6pbF7RyuS1qb+uz2Hw5sBE4AjgYOSfL+xe0crktam/rs9v8q8J9V9XRVvQhcD7xtmLIkja1P+B8DzkxycJIwG65r+zBlSRpbn8/8W5gNzrkV2Na91hUD1SVpZH2H6/oE8ImBapE0Ic/wkxrVd5Reaa+7/x1fXPVzfv5bHxihkvXFLb/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjvLBHo5jnYpspTV3fWryQyC2/1CjDLzXK8EuN2mP4k1yVZFeSexc8dkSSW5M80N0ePm6Zkoa2ki3/3wPnLnpsM3BbVZ0E3Nbdl7SO7DH8VfVN4JlFD28Eru6mrwbeO2xZksY272f+11XVToDu9rXLNXS4LmltGv2An8N1SWvTvOF/KslRAN3truFKkjSFecN/I7Cpm94EfHWYciRNZSVf9V0L/DtwcpIdST4E/DFwTpIHgHO6+5LWkT2e219VFy0z6+yBa5E0Ic/wkxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGrYvhut7wvm2rfs6jX3rzCJW0Z60Pu7VezLMOj80tv9Qowy81yvBLjZp3uK5PJ7k/yT1Jbkhy2KhVShrcvMN13Qq8qap+Efge8PGB65I0srmG66qqW6rqpe7u7cCxI9QmaURDfOb/IHDzcjMdrktam3qFP8nlwEvANcu1cbguaW2a+ySfJJuA84Gzq6qGK0nSFOYKf5JzgY8Bv1xVPxq2JElTmHe4rr8BDgVuTXJXks+PXKekgc07XNeVI9QiaUKe4Sc1KlMeq3tNjqi3ZG2P7+nVgMPYV68GfPfRp+7tEnZrS93Gc/VMVtLWLb/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UqHUxVt+UHBewDWv96rwpuOWXGmX4pUbNNVzXgnkfSVJJjhynPEljmXe4LpIcB5wDPDZwTZImMNdwXZ0/Bz4K+Jv90jo012f+JBcAT1TV3Sto63Bd0hq06q/6khwMXA68ayXtq+oK4AqY/YDnapcnaRzzbPnfCJwA3J3kEWYj9G5N8vohC5M0rlVv+atqG/Dal+93fwA2VNV/DViXpJHNO1yXpHVu3uG6Fs4/frBqJE3GM/ykRnlhzwDmuRhoX/duTt3bJWgP3PJLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjUrVdD+rl+Rp4NFlZh8JrIVfA7KOV7KOV1rrdbyhqn5uJS8wafh3J8kdVbXBOqzDOqapw91+qVGGX2rUWgr/FXu7gI51vJJ1vNI+U8ea+cwvaVpracsvaUKGX2rUpOFPcm6S7yZ5MMnmJeYnyV918+9JcvoINRyX5OtJtie5L8mlS7R5Z5Jnk9zV/fujoetYsKxHkmzrlnPHEvNH7ZMkJy/4f96V5Lkkly1qM1p/JLkqya4k9y547IgktyZ5oLs9fJnn7nZ9GqCOTye5v+v3G5Ictsxzd/seDlDHJ5M8saD/z1vmuavrj6qa5B+wH/AQcCJwIHA3cMqiNucBNwMBzgS2jFDHUcDp3fShwPeWqOOdwNcm6pdHgCN3M3/0Pln0Hn2f2Ykik/QHcBZwOnDvgsf+BNjcTW8GPjXP+jRAHe8C9u+mP7VUHSt5Dweo45PAR1bw3q2qP6bc8p8BPFhVD1fVC8B1wMZFbTYC/1AztwOHJTlqyCKqamdVbe2mfwBsB44ZchkDG71PFjgbeKiqljsLc3BV9U3gmUUPbwSu7qavBt67xFNXsj71qqOqbqmql7q7tzMblHZUy/THSqy6P6YM/zHA4wvu7+AnQ7eSNoNJcjxwGrBlidlvTXJ3kpuT/MJYNQAF3JLkziQXLzF/yj65ELh2mXlT9QfA66pqJ8z+WLNgYNgFJl1XgA8y2wNbyp7ewyFc0n38uGqZj0Gr7o8pw58lHlv8PeNK2gwiyauBrwCXVdVzi2ZvZbbr+0vAXwP/PEYNnbdX1enAe4DfTXLW4lKXeM7gfZLkQOAC4J+WmD1lf6zUlOvK5cBLwDXLNNnTe9jX54A3AqcCO4E/W6rMJR7bbX9MGf4dwHEL7h8LPDlHm96SHMAs+NdU1fWL51fVc1X1P930TcABSY4cuo7u9Z/sbncBNzDbfVtokj5htuJuraqnlqhxsv7oPPXyR5vudtcSbaZaVzYB5wO/Wd2H68VW8B72UlVPVdX/VdWPgb9d5vVX3R9Thv/bwElJTui2MhcCNy5qcyPwW90R7jOBZ1/e/RtKkgBXAtur6jPLtHl9144kZzDrp/8eso7utQ9JcujL08wOMN27qNnofdK5iGV2+afqjwVuBDZ105uAry7RZiXrUy9JzgU+BlxQVT9aps1K3sO+dSw8xvPry7z+6vtjiCOUqziSeR6zo+sPAZd3j30Y+HA3HeCz3fxtwIYRangHs92he4C7un/nLarjEuA+ZkdMbwfeNlJ/nNgt4+5ueXurTw5mFuafWfDYJP3B7A/OTuBFZluvDwE/C9wGPNDdHtG1PRq4aXfr08B1PMjsc/TL68nnF9ex3Hs4cB1f7N77e5gF+qgh+sPTe6VGeYaf1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuN+n9VwdldSoRlqgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_temp = y_raw.reshape(ndata,16,16)\n",
    "plt.imshow(y_temp[1,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_raw, y_raw, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train1 = x_train.reshape(int(0.8*ndata),16,8,1)\n",
    "x_test1 = x_test.reshape(int(0.2*ndata),16,8,1)\n",
    "\n",
    "y_train1 = y_train.reshape(int(0.8*ndata),16,16)\n",
    "y_test1 = y_test.reshape(int(0.2*ndata),16,16)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train1, y_train1), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test1, y_test1), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8000, 16, 16]), torch.Size([2000, 16, 16]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train1.shape, y_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8000, 16, 8, 1]), torch.Size([2000, 16, 8, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train1.shape, x_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2368130\n",
      "shape before concat torch.Size([10, 16, 8, 3])\n",
      "shape after concat torch.Size([10, 16, 8, 32])\n",
      "before FFT torch.Size([10, 32, 16, 8]) after FFT torch.Size([10, 32, 16, 5])\n",
      "bixy,ioxy->boxy: input is torch.Size([10, 32, 12, 5]) weights are torch.Size([32, 32, 12, 12])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): operands do not broadcast with remapped shapes [original->remapped]: [10, 32, 12, 5]->[10, 1, 12, 5, 32] [32, 32, 12, 12]->[1, 32, 12, 12, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\azhir\\Desktop\\final_results\\fno2_attempt2.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mcuda(), y\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m out \u001b[39m=\u001b[39m model(x)\u001b[39m.\u001b[39mreshape(batch_size, s, s)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m#out = y_normalizer.decode(out)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m#y = y_normalizer.decode(y)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss \u001b[39m=\u001b[39m myloss(out\u001b[39m.\u001b[39mview(batch_size,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), y\u001b[39m.\u001b[39mview(batch_size,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\azhir\\anaconda3\\envs\\gundam\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\azhir\\Desktop\\final_results\\fno2_attempt2.ipynb Cell 12\u001b[0m in \u001b[0;36mFNO2d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m#x = F.pad(x, [0,self.padding, 0,self.padding])\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv0(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw0(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m x \u001b[39m=\u001b[39m x1 \u001b[39m+\u001b[39m x2\n",
      "File \u001b[1;32mc:\\Users\\azhir\\anaconda3\\envs\\gundam\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\azhir\\Desktop\\final_results\\fno2_attempt2.ipynb Cell 12\u001b[0m in \u001b[0;36mSpectralConv2d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Multiply relevant Fourier modes\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m out_ft \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(batchsize, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_channels,  x\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m), x\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcfloat, device\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m out_ft[:, :, :\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodes1, :\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodes2] \u001b[39m=\u001b[39m \\\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompl_mul2d(x_ft[:, :, :\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodes1, :\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodes2], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m out_ft[:, :, \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodes1:, :\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodes2] \u001b[39m=\u001b[39m \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompl_mul2d(x_ft[:, :, \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodes1:, :\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodes2], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m#Return to physical space\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\azhir\\Desktop\\final_results\\fno2_attempt2.ipynb Cell 12\u001b[0m in \u001b[0;36mSpectralConv2d.compl_mul2d\u001b[1;34m(self, input, weights)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mbixy,ioxy->boxy: input is\u001b[39m\u001b[39m'\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape, \u001b[39m'\u001b[39m\u001b[39mweights are\u001b[39m\u001b[39m'\u001b[39m, weights\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49meinsum(\u001b[39m\"\u001b[39;49m\u001b[39mbixy,ioxy->boxy\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39minput\u001b[39;49m, weights)\n",
      "File \u001b[1;32mc:\\Users\\azhir\\anaconda3\\envs\\gundam\\lib\\site-packages\\torch\\functional.py:330\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[39m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[0;32m    327\u001b[0m     \u001b[39m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m--> 330\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): operands do not broadcast with remapped shapes [original->remapped]: [10, 32, 12, 5]->[10, 1, 12, 5, 32] [32, 32, 12, 12]->[1, 32, 12, 12, 32]"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# training and evaluation\n",
    "################################################################\n",
    "model = FNO2d(modes, modes, width).cuda()\n",
    "print(count_params(model))\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "#y_normalizer.cuda()\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    t1 = default_timer()\n",
    "    train_l2 = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x).reshape(batch_size, s, s)\n",
    "        #out = y_normalizer.decode(out)\n",
    "        #y = y_normalizer.decode(y)\n",
    "\n",
    "        loss = myloss(out.view(batch_size,-1), y.view(batch_size,-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_l2 += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_l2 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "\n",
    "            out = model(x).reshape(batch_size, s, s)\n",
    "            #out = y_normalizer.decode(out)\n",
    "\n",
    "            test_l2 += myloss(out.view(batch_size,-1), y.view(batch_size,-1)).item()\n",
    "\n",
    "    train_l2/= ntrain\n",
    "    test_l2 /= ntest\n",
    "\n",
    "    t2 = default_timer()\n",
    "    print(ep, t2-t1, train_l2, test_l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes1 = modes2 = modes\n",
    "in_channels = out_channels = width\n",
    "scale = (1 / (in_channels * out_channels))\n",
    "\n",
    "weights1 = nn.Parameter(scale * torch.rand(in_channels, out_channels, modes1, modes2, dtype=torch.cfloat))\n",
    "weights2 = nn.Parameter(scale * torch.rand(in_channels, out_channels, modes1, modes2, dtype=torch.cfloat))\n",
    "\n",
    "# Complex multiplication\n",
    "def compl_mul2d(input, weights):\n",
    "    print('bixy,ioxy->boxy: input is', input.shape, 'weights are', weights.shape)\n",
    "    # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "    return torch.einsum(\"bixy,ioxy->boxy\", input, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before FFT torch.Size([10, 16, 8, 1]) after FFT torch.Size([10, 16, 8, 1]) out_ft shape torch.Size([10, 32, 8, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batchsize = x.shape[0]\n",
    "#Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "x_ft = torch.fft.rfft2(x)\n",
    "\n",
    "# Multiply relevant Fourier modes\n",
    "out_ft = torch.zeros(batchsize, out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat)\n",
    "\n",
    "print('before FFT',x.shape,'after FFT',x_ft.shape, 'out_ft shape',out_ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bixy,ioxy->boxy: input is torch.Size([10, 16, 8, 1]) weights are torch.Size([32, 32, 12, 12])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): operands do not broadcast with remapped shapes [original->remapped]: [10, 16, 8, 1]->[10, 1, 8, 1, 16] [32, 32, 12, 12]->[1, 32, 12, 12, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\azhir\\Desktop\\final_results\\fno2_attempt2.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m out_ft[:, :, :modes1, :modes2] \u001b[39m=\u001b[39m \\\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     compl_mul2d(x_ft[:, :, :modes1, :modes2], weights1)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m out_ft[:, :, \u001b[39m-\u001b[39mmodes1:, :modes2] \u001b[39m=\u001b[39m \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     compl_mul2d(x_ft[:, :, \u001b[39m-\u001b[39mmodes1:, :modes2], weights2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#Return to physical space\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\azhir\\Desktop\\final_results\\fno2_attempt2.ipynb Cell 17\u001b[0m in \u001b[0;36mcompl_mul2d\u001b[1;34m(input, weights)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mbixy,ioxy->boxy: input is\u001b[39m\u001b[39m'\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape, \u001b[39m'\u001b[39m\u001b[39mweights are\u001b[39m\u001b[39m'\u001b[39m, weights\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt2.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49meinsum(\u001b[39m\"\u001b[39;49m\u001b[39mbixy,ioxy->boxy\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39minput\u001b[39;49m, weights)\n",
      "File \u001b[1;32mc:\\Users\\azhir\\anaconda3\\envs\\gundam\\lib\\site-packages\\torch\\functional.py:330\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[39m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[0;32m    327\u001b[0m     \u001b[39m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m--> 330\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): operands do not broadcast with remapped shapes [original->remapped]: [10, 16, 8, 1]->[10, 1, 8, 1, 16] [32, 32, 12, 12]->[1, 32, 12, 12, 32]"
     ]
    }
   ],
   "source": [
    "out_ft[:, :, :modes1, :modes2] = \\\n",
    "    compl_mul2d(x_ft[:, :, :modes1, :modes2], weights1)\n",
    "out_ft[:, :, -modes1:, :modes2] = \\\n",
    "    compl_mul2d(x_ft[:, :, -modes1:, :modes2], weights2)\n",
    "\n",
    "#Return to physical space\n",
    "x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('gundam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cddc3d62f9a7e02ecb46a81ab11648d63c0573e5e13f25e036352996103f8463"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
