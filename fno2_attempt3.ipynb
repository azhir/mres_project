{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial set-up\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "from timeit import default_timer\n",
    "from utilities3 import *\n",
    "\n",
    "from Adam import Adam\n",
    "from sewar.full_ref import rmse, uqi\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: measurements torch.Size([8000, 128]) solution torch.Size([8000, 16, 16, 1])\n",
      "training data: measurements torch.Size([2000, 128]) solution torch.Size([2000, 16, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "# DATA LOADING\n",
    "############################## DATA LOADING ########################################\n",
    "raw_data = scipy.io.loadmat('./datasets/training_data_16x16_10k')\n",
    "\n",
    "sol = np.asarray(raw_data['solution_data'])\n",
    "mes = np.asarray(raw_data['measurement_data'])\n",
    "\n",
    "\n",
    "def convert_data(data_x, data_y):\n",
    "    data_X = torch.from_numpy(data_x).float()\n",
    "    data_Y = torch.from_numpy(data_y).float()\n",
    "    return data_X, data_Y\n",
    "\n",
    "# we are solving the inverse problem, so going from measurements, to solutions\n",
    "X, y = convert_data(mes, sol)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ntrain = X_train.shape[0]\n",
    "ntest = X_test.shape[0]\n",
    "\n",
    "X_dim = X_train.shape[1]\n",
    "y_dim = y_train.shape[1]\n",
    "\n",
    "X_train = X_train.reshape(ntrain,X_dim)\n",
    "X_test = X_test.reshape(ntest,X_dim)\n",
    "\n",
    "y_train = y_train.reshape(ntrain,16,16,1)\n",
    "y_test = y_test.reshape(ntest,16,16,1)\n",
    "\n",
    "train_dataset = TensorDataset( X_train, y_train )\n",
    "test_dataset = TensorDataset( X_test, y_test )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('training data: measurements', X_train.shape, 'solution', y_train.shape)\n",
    "print('training data: measurements', X_test.shape, 'solution', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL METRICS\n",
    "###################################\n",
    "# MODEL METRICS\n",
    "###################################\n",
    "def model_metrics(model,test_loader,ntrain):\n",
    "    rms_values, psnr_values, ssim_values, uqi_values = [], [], [], []\n",
    "\n",
    "    for test_num in range(ntest):\n",
    "        K = torch.unsqueeze(test_loader.dataset[test_num][0], 0).cuda()\n",
    "        model.eval()\n",
    "        predicted_np = np.reshape(model(K).detach().cpu().numpy(),(16,16))\n",
    "\n",
    "        truth = test_loader.dataset[test_num][1]\n",
    "        truth_np = np.reshape(truth.numpy(),(16,16))\n",
    "\n",
    "        #diff_image = predicted - truth_np\n",
    "        #np.sqrt(np.sum(diff_image**2)/256)\n",
    "\n",
    "        rms_values.append(rmse(predicted_np, truth_np))\n",
    "        psnr_values.append(psnr(truth_np, predicted_np, data_range=predicted_np.max() - predicted_np.min()))\n",
    "        ssim_values.append(ssim(truth_np, predicted_np, data_range=predicted_np.max() - predicted_np.min()))\n",
    "        uqi_values.append(uqi(predicted_np, truth_np))\n",
    "\n",
    "    model_rms = sum(rms_values)/ len(rms_values)\n",
    "    std_rms = np.std(np.array(rms_values))\n",
    "\n",
    "    model_psnr = sum(psnr_values)/ len(psnr_values)\n",
    "    std_psnr = np.std(np.array(psnr_values))\n",
    "\n",
    "    model_ssim = sum(ssim_values)/ len(ssim_values)\n",
    "    std_ssim = np.std(np.array(ssim_values))\n",
    "\n",
    "    model_uqi = sum(uqi_values)/ len(uqi_values)\n",
    "    std_uqi = np.std(np.array(uqi_values))\n",
    "\n",
    "\n",
    "    print(\"RMSE: \", model_rms, std_rms, sep=\"---\")\n",
    "    print(\"PSNR: \", model_psnr, std_psnr, sep=\"---\")\n",
    "    print(\"SSIM: \", model_ssim, std_ssim, sep=\"---\")\n",
    "    print(\"UQI: \", model_uqi, std_uqi, sep=\"---\")\n",
    "\n",
    "\n",
    "    output = {\n",
    "    \"rms\": rms_values,\n",
    "    \"psnr\": psnr_values,\n",
    "    \"ssim\": ssim_values,\n",
    "    \"uqi\": uqi_values,\n",
    "    }\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CONFIGURATIONS - HYPER PARAMETERS\n",
    "################################################################\n",
    "#  configurations - HYPER PARAMETERS\n",
    "################################################################\n",
    "epochs = 20\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 100\n",
    "step_size = 50\n",
    "gamma = 0.5\n",
    "\n",
    "width = 64\n",
    "modes = 5\n",
    "\n",
    "\n",
    "weight_decay=1e-4\n",
    "\n",
    "# Hyperparameters to change - epoch{10,100,200}, step_size{}, batchsize{}, gamma{}, modes{}, weight_decay{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# fourier layer\n",
    "################################################################\n",
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        print('bixy,ioxy->boxy: input is', input.shape, 'weights are', weights.shape)\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "        print('before FFT',x.shape,'after FFT',x_ft.shape)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNO2d_edit(nn.Module):\n",
    "    def __init__(self, modes1, modes2,  width):\n",
    "        super(FNO2d_edit, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.padding = 9 # pad the domain if input is non-periodic\n",
    "\n",
    "        self.fc00 = nn.Linear(8*16, 16*16)\n",
    "        self.fc0 = nn.Linear(3, self.width) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        #print('shape before concat', x.shape)\n",
    "        x = self.fc00(x)\n",
    "        x = x.reshape(x.shape[0],16,16,1)\n",
    "\n",
    "        \n",
    "        x = self.fc0(x)\n",
    "        #print('shape after concat', x.shape)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        #x = F.pad(x, [0,self.padding, 0,self.padding])\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        #x = x[..., :-self.padding, :-self.padding]\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND EVALUATION\n",
    "################################################################\n",
    "# TRAINING AND EVALUATION\n",
    "################################################################\n",
    "\n",
    "\n",
    "def train_model(model, epochs, batch_size, learning_rate, weight_decay, step_size, gamma):\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    train_losses, test_losses, epoch_time = [], [], []\n",
    "\n",
    "    myloss = LpLoss(size_average=False)\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        t1 = default_timer()\n",
    "        train_mse = 0\n",
    "        train_l2 = 0\n",
    "        total_time = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            #print('in: ',x.shape)\n",
    "            out = model(x)\n",
    "            #print('out: ',out.shape)\n",
    "\n",
    "            mse = F.mse_loss(out.view(batch_size, -1), y.view(batch_size, -1), reduction='mean')\n",
    "            l2 = myloss(out.view(batch_size, -1), y.view(batch_size, -1))\n",
    "            l2.backward() # use the l2 relative loss\n",
    "\n",
    "            optimizer.step()\n",
    "            train_mse += mse.item()\n",
    "            train_l2 += l2.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        model.eval()\n",
    "        test_l2 = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "\n",
    "                out = model(x)\n",
    "                test_l2 += myloss(out.view(batch_size, -1), y.view(batch_size, -1)).item()\n",
    "\n",
    "        #train_mse /= len(train_loader)\n",
    "        train_l2 /= ntrain\n",
    "        train_losses.append(train_l2)\n",
    "        test_l2 /= ntest\n",
    "        test_losses.append(test_l2)\n",
    "\n",
    "\n",
    "        t2 = default_timer()\n",
    "        epoch_time.append(t2 - t1)\n",
    "\n",
    "        print(ep, t2 - t1, train_l2, test_l2)\n",
    "    total_time = sum(epoch_time)\n",
    "    print(\"TOTAL TIME: \", total_time, sep=\"---\")\n",
    "\n",
    "    output = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"epoch_time\": epoch_time,\n",
    "    \"training_time\": total_time,\n",
    "    \"model\": model\n",
    "    }\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x223c0825000>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATQAAAD4CAYAAABi3BrkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAV+klEQVR4nO3dfbAddX3H8feHEIhB0pjGh5hkJGjKNGWqZFJIxXGsiE0iQ+yM0wFUKDq9kymx4OhgLNPqn4qtIi1D5iop0AKp5aFm7LUBqQ7jjMkkpCEQI3BJrVyIREQBZSTJvd/+sXvb83jv3nP27Dm79/Nidu45u789+82ew3d+u7+HVURgZlYFJ/U7ADOzvDihmVllOKGZWWU4oZlZZTihmVllnFzkwU7RqTGP04o8pNms8ht+zbF4Vd18xh//0Wnx8xfGM5V9+MCrOyNiXTfHy1OhCW0ep3GeLijykGazyu54sOvPeP6FcXbvXJap7NwlTy3u+oA5KjShmVkZBOMx0e8gOuKEZmZ1ApignB3uu2oUkLRO0uOSRiVtySsoM+uviYz/DZqOa2iS5gA3ARcCY8AeSTsi4od5BWdmxQuC4yW95OymhnYuMBoRhyPiGLAd2JhPWGbWLwGME5mWQdPNPbSlwNM178eA8xoLSRoChgDmMb+Lw5lZUcp6D62bhNaqr0vTWYiIYWAYYIEWlfMsmc0iAYyXdBaebhLaGLC85v0y4NnuwjGzQVDOO2jdJbQ9wEpJK4BngEuAy3KJysz6Jgb0/lgWHSe0iDghaTOwE5gDbIuIg7lFZmZ9EQHHy5nPuutYGxEjwEhOsZjZQBDjLW+RDz6PFDCzOgFMzMYamiVGb1jb7xBsht52za5+hzDQXEMzs0pIOtY6oZlZBQRwPMo596sTmpnVCcR4SSezdkIzsyYT4UtOM6sA30MzswoR476HZmZVkMxY64RmZhUQIY7FnH6H0REnNDNrMuF7aGZWBUmjgC85zawS3ChgZhXhRoEK8UDz2aHT73m2DGofd8daM6uCQByPcqaGckZtZj3jRgEzq4xApb3k7DgNS1ou6buSDkk6KOnqPAMzs/6Z4KRMy6DppoZ2AvhUROyTdDrwsKQHIuKHOcVmZn0QwezrthERR4Aj6euXJR0ieZq6E5pZiSWNArN46JOkM4BzgN0ttg0BQwDzmJ/H4cysx2Zto4Ck1wL3ANdExEuN2yNiGBgGWKBFJX2WjNnsEWh2TvAoaS5JMrsjIu7NJyQz67dZV0OTJOAW4FBEfDm/kMysn5LncpYzoXUT9fnAR4H3StqfLhtyisvM+iZ5cnqWJdOnSeskPS5pVNKWFtsl6cZ0+wFJqxu2z5H0X5K+Nd2xumnl/D6UdNIkM2sreYxdPq2ckuYANwEXAmPAHkk7Grp3rQdWpst5wM3p30lXA4eABdMdr5z1SjPrmQgxESdlWjI4FxiNiMMRcQzYDmxsKLMRuD0Su4CFkpYASFoGfAD4epaDVXbok2fNsF646sknOtrvppW/k3MkvTWDjrWLJe2teT+c9myYtBR4uub9GPW1r3ZllpL0c70BuBY4PUswlU1oZtaZZD60zHeTno+INVNsb/VBjd23WpaRdBFwNCIelvSeLME4oZlZg1xnrB0Dlte8XwY8m7HMh4CL08bGecACSf8cER9pdzDfQzOzOkm3DWVaMtgDrJS0QtIpwCXAjoYyO4DL09bOtcCLEXEkIj4bEcsi4ox0v/+cKpmBa2hm1iDPsZwRcULSZmAnMAfYFhEHJW1Kt28FRoANwCjwCnBlp8dzQjOzJnlODRQRIyRJq3bd1prXAVw1zWd8D/jedMdyQjOzOsn0QeXsYuqEZmZNZuXgdDOrnmS2jXK2FzqhmVmdZOiTE5qZVYJraGZWITMYKTBQnNDMrI5bOXvMA82t7DoZ1N7PAe2+5DSzSpi1zxQws+oJ4IRraGZWFbP2kjOdYncv8ExEXNR9SGbWV9ln0hg4edTQMs/3bWaDb4YTPA6UruqVM53v28zKIcf50ArVbQ3tBmYw37eZDb7JCR7LqJsHDWea71vSEDAEMI/5nR7OzAoSiBMTs69R4HwyzPedPgFmGGCBFjU+HMHMBtCsu4fWyXzfZlYCMXvvoZlZxczKe2i1ss73bWblMKsTmplVRyDGZ2GjgJlVVFkbBZzQzKxOhC85zaxCwgnNzKphMLtkZOGEZmZNXEMzs0qIgPEJJzQzqwi3cppZJQS+5DSzynCjgJlVSJR0XhwnNDNr4ktOM6uEpJXTYznNrCJ8yWlmleFLTquk6zfcWdixrh25rLBjWXuBck1oktYBXwXmAF+PiC80bFe6fQPwCvBnEbFP0jzgIeBUklx1d0R8bqpjlfNC2cx6KjIu00kfRH4TsB5YBVwqaVVDsfXAynQZAm5O178KvDci3g68A1gnae1Ux3NCM7N6ATGhTEsG5wKjEXE4Io4B24GNDWU2ArdHYhewUNKS9P2v0jJz02XKPOqEZmZNIpRpARZL2luzDDV81FLg6Zr3Y+m6TGUkzZG0HzgKPBARu6eKu6t7aJIWkjw1/WySzPmxiPhBN59pZv03g1bO5yNizRTbW1XjGj+9bZmIGAfekeaa+ySdHRGPtTtYt40CXwX+IyI+JOkU8JOEzcou57GcY8DymvfLgGdnWiYifinpe8A6oG1C6/iSU9IC4N3ALekBj0XELzv9PDMbEAGEsi3T2wOslLQirfRcAuxoKLMDuFyJtcCLEXFE0uvTmhmSXgO8D/jRVAfrpoZ2JvAz4B8lvR14GLg6In5dWyi9ph4CmOcKnFkp5NWxNiJOSNoM7CTptrEtIg5K2pRu3wqMkHTZGCXptnFluvsS4La0pfQk4BsR8a2pjtdNQjsZWA18IiJ2S/oqsAX464Z/0DAwDLBAi0ra/9hsNsncgplJRIyQJK3adVtrXgdwVYv9DgDnzORY3bRyjgFjNa0Od5MkODMru7w6ohWs44QWET8FnpZ0VrrqAuCHuURlZv0TM+q2MVC6beX8BHBHerPvMP9/7WtmZTaAta8sukpoEbEfmKoPipmV0uDVvrLw4PQSKnLAeJE6/Xd5UHsPTPQ7gM44oZlZvcl+aCXkhGZmTTzBo5lVhxOamVWGLznNrCrkGpqZVUIIchz6VCQnNDNr5hqamVWGE5qZVYYTmplVgjvWmlmVuJXTzKrDCc3MqsI1NJuxqs6aUbROzqNn6JiG76GZWSUM6PTaWTihmVkzJzQzqwqVdILHbp76hKRPSjoo6TFJd0mal1dgZtZHs+2pT5KWAn8JrImIs0keInpJXoGZWX8osi+DpttLzpOB10g6DswHnu0+JDPru5K2cnbzXM5ngL8FfgIcAV6MiPsby0kakrRX0t7jvNp5pGZWnFl4yfk6YCOwAngzcJqkjzSWi4jhiFgTEWvmcmrnkZpZYcp6ydlNo8D7gP+OiJ9FxHHgXuCd+YRlZn0TSStnlmXQdJPQfgKslTRfkoALgEP5hGVmfTXbLjkjYjdwN7APeDT9rOGc4jKzfippQuuqlTMiPgd8LqdYzGxADOL9sSy66lhrZjZIPPTJbAY++e+Xd7TfVz5we86R9FhJa2hOaGZWLwazBTMLJzQza1bSGprvoZlZHZFvx1pJ6yQ9LmlU0pYW2yXpxnT7AUmr0/XLJX1X0qF0EoyrpzuWE5qZNcup24akOcBNwHpgFXCppFUNxdYDK9NlCLg5XX8C+FRE/C6wFriqxb51nNDMrF6+s22cC4xGxOGIOAZsJxkyWWsjcHskdgELJS2JiCMRsQ8gIl4m6bi/dKqDOaGZWbOJjMv0lgJP17wfozkpTVtG0hnAOcDuqQ7mRgEzazKDjrWLJe2teT8cEbUjhlrNQ9T46VOWkfRa4B7gmoh4aapgnNDMrFn2hPZ8RKyZYvsYsLzm/TKa501sW0bSXJJkdkdE3DtdML7kNLN6WRsEsiW9PcBKSSsknUIyq/WOhjI7gMvT1s61JHMrHkknvbgFOBQRX85yMNfQzKxJXmM5I+KEpM3ATpJp+rdFxEFJm9LtW4ERYAMwCrwCXJnufj7wUeBRSfvTdX8VESPtjueEZmbNcuxYmyagkYZ1W2teB3BVi/2+T+v7a205oZlZEw99MiuR6zfc2dF+145c1tF+nQxqfxu7OjpW1wZ0rrMsnNDMrI6Y4XXeAHFCM7NmrqGZWVVUdsZaSdskHZX0WM26RZIekPRk+vd1vQ3TzApV0mcKZOlYeyuwrmHdFuDBiFgJPJi+N7MqqPJj7CLiIeCFhtUbgdvS17cBH8w3LDPrq5LW0Dq9h/bGiDgCkA5ReEO7gpKGSOY4Yh7zOzycmRWpsvfQuhURwxGxJiLWzOXUXh/OzPJQ0hpapwntOUlLANK/R/MLycz6Lc8puIvUaULbAVyRvr4C+GY+4ZhZ3wV5TvBYqCzdNu4CfgCcJWlM0seBLwAXSnoSuDB9b2YVkPdDUoo0baNARFzaZtMFOcdiZoNiAJNVFh4pYGZNFOXMaE5oOXjqT7dOX6iFe361IOdILKtOZ82YFQa0BTMLJzQzazKI98eycEIzsyaDOKwpCyc0M2vmGpqZVcKAdsnIwgnNzJo5oZlZFUx2rC0jJzQza6KJcmY0JzQzq+d+aGZWJe62YWbV4RqamVWFGwXMrBoC8OD03nnbNbtmvM/oDWt7EElrb/3Gpo72u37DnTlHYoOqk99wP/kemplVgvuhmVl1RPiS08yqo6w1tCzPFNgm6aikx2rWfUnSjyQdkHSfpIU9jdLMilXhx9jdCqxrWPcAcHZE/D7wBPDZnOMysz4q60NSpk1oEfEQ8ELDuvsj4kT6dhewrAexmVk/BDAe2ZYBk8c9tI8B/9Juo6QhYAhgHvNzOJyZ9dog1r6y6PRBwwBIug44AdzRrkxEDEfEmohYM5dTuzmcmRVlsqVzuiUDSeskPS5pVNKWFtsl6cZ0+wFJq2u2Nd3Dn0rHCU3SFcBFwIcjStrGa2Yt5XUPTdIc4CZgPbAKuFTSqoZi64GV6TIE3Fyz7Vaa7+G31VFCk7QO+AxwcUS80slnmNmAytrCma0acy4wGhGHI+IYsB3Y2FBmI3B7JHYBCyUtgdb38KeSpdvGXcAPgLMkjUn6OPAPwOnAA5L2S+rswZRmNnAEaDwyLcBiSXtrlqGGj1sKPF3zfixdN9MymUzbKBARl7ZYfUsnBzOzcpjBk9Ofj4g1U31Ui3WNH56lTCYeKWBm9fLtNDsGLK95vwx4toMymVQ2oXU6u0GRs3RcO3JZR/t5lo56fzCvo99+R8o2a0Znch3LuQdYKWkF8AxwCdD4w98BbJa0HTgPeDEijnRysK66bZhZNeXVypl2wN8M7AQOAd+IiIOSNkmanHdrBDgMjAJfA/7i/+JofQ+/rcrW0MysCzn2xIqIEZKkVbtua83rAK5qs2+re/htOaGZWb1gsgWzdJzQzKxZOfOZE5qZNZtBt42B4oRmZs2c0MysEgLwQ1LMrApE+JLTzCpkopxVNCc0M6vnS04zqxJfcppZdTihmVk1+EHDldHJbApFztABxc4usec3by7sWJ41Y0BMPvWphJzQzKyJ76GZWXWUNKFleaZA28dISfq0pJC0uDfhmVnhApiIbMuAyTLB4620eIyUpOXAhcBPco7JzPoq4zM5B7AWN21Cm+IxUl8BrqW0E42YWVslTWgd3UOTdDHwTEQ8IrV6YEtd2SGSh4cyj/mdHM7MihTAeDmHCsw4oUmaD1wHvD9L+YgYBoYBFmjR4KV0M2sQEOVMaJ08JOWtwArgEUk/Jnnk1D5Jb8ozMDPro9lyyRkRjwJvmHyfJrU1EfF8jnGZWb9MtnKWUJZuGzN6jJSZVUBVa2jTPUYqIs7ILRozGwwDmKyy8EgBM6sXAePj/Y6iI05oOSh6oPOfX/OuQo9XlGHO7HcINsk1NDOrDCc0M6uGwRynmYUTmpnVC4iSdqx1QjOzZrNl6JOZVVyEH2NnZhXiRgEzq4pwDc3MqmEwhzVl4YRmZvVKPDjdCc3M6gQQJR361Ml8aGZWZZFO8JhlyUDSOkmPSxqVtKXFdkm6Md1+QNLqrPs2ckIzsyYxEZmW6UiaA9wErAdWAZdKWtVQbD2wMl2GgJtnsG8dJzQza5ZfDe1cYDQiDkfEMWA7sLGhzEbg9kjsAhZKWpJx3zqF3kN7mV88/524+3/abF4MDMKst46jnuOoN+hxvKXbD36ZX+z8Ttyd9Vm78yTtrXk/nD5HZNJS4Oma92PAeQ2f0arM0oz71ik0oUXE69ttk7Q3ItYUGY/jcByOo1lEND2HtwutHgvXeK3arkyWfeu4ldPMemkMWF7zfhnwbMYyp2TYt47voZlZL+0BVkpaIekU4BJgR0OZHcDlaWvnWuDFiDiScd86g1RDG56+SCEcRz3HUc9xzEBEnJC0GdgJzAG2RcRBSZvS7VuBEWADMAq8Alw51b5THU9R0iEOZmaNfMlpZpXhhGZmlVFoQutmCESOMSyX9F1JhyQdlHR1izLvkfSipP3p8jd5x1FzrB9LejQ9zt4W23t6TiSdVfPv3C/pJUnXNJTp2fmQtE3SUUmP1axbJOkBSU+mf1/XZt8ZDYvpII4vSfpRet7vk7Swzb5Tfoc5xPF5Sc/UnP8NbfbN7XyUVkQUspDc1HsKOJOkOfYRYFVDmQ3At0n6n6wFdvcgjiXA6vT16cATLeJ4D/Ctgs7Lj4HFU2zv+Tlp+I5+CrylqPMBvBtYDTxWs+56YEv6egvwxU5+TznE8X7g5PT1F1vFkeU7zCGOzwOfzvDd5XY+yroUWUPrZghEbiLiSETsS1+/DBwi6ZE8qHp+TmpcADwVEe1Gc+QuIh4CXmhYvRG4LX19G/DBFrvOeFjMTOOIiPsj4kT6dhdJP6ieanM+ssj1fJRVkQmt3fCGmZbJjaQzgHOA3S02/6GkRyR9W9Lv9SoGkp7P90t6WNJQi+1FnpNLgLvabCvqfAC8MZJ+SKR/39CiTKG/FeBjJDXlVqb7DvOwOb303dbmErzo8zGQikxo3QyByJ2k1wL3ANdExEsNm/eRXHa9Hfh74N96EUPq/IhYTTKjwFWS3t0Yaot9cj8nacfFi4F/bbG5yPORVZG/leuAE8AdbYpM9x1262bgrcA7gCPA37UKs8W6Wdcnq8iE1s0QiFxJmkuSzO6IiHsbt0fESxHxq/T1CDBXUtbBujMSEc+mf48C95FcOtQq5JyQ/M+4LyKeaxFjYecj9dzkZXX692iLMkX9Vq4ALgI+HOnNqkYZvsOuRMRzETEeycMyv9bm84v6nQy0IhNaN0MgciNJwC3AoYj4cpsyb0rLIelckvP08zzjSD/7NEmnT74muQn9WEOxnp+T1KW0udws6nzU2AFckb6+AvhmizIzHhYzU5LWAZ8BLo6IV9qUyfIddhtH7T3TP2nz+T0/H6VQZAsESYvdEyStMdel6zYBm9LXIpnQ7SngUWBND2J4F0lV/ACwP102NMSxGThI0lK0C3hnj87HmekxHkmP169zMp8kQf1WzbpCzgdJEj0CHCepZXwc+G3gQeDJ9O+itOybgZGpfk85xzFKcl9q8neytTGOdt9hznH8U/rdHyBJUkt6fT7Kunjok5lVhkcKmFllOKGZWWU4oZlZZTihmVllOKGZWWU4oZlZZTihmVll/C9v4aDzLGeFoQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Actual\n",
    "test_num = 434\n",
    "truth_out = test_loader.dataset[test_num][1]\n",
    "plt.imshow(np.reshape(truth_out.numpy(),(16,16)))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1696769\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\azhir\\Desktop\\final_results\\fno2_attempt3.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model_fcn_sigma \u001b[39m=\u001b[39m FNO2d_edit(modes, modes, width)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(count_params(model_fcn_sigma))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m output_sigma \u001b[39m=\u001b[39m train_model(model_fcn_sigma, \u001b[39m20\u001b[39;49m, batch_size, learning_rate, weight_decay, step_size, gamma)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m output_sigma_model \u001b[39m=\u001b[39m output_sigma\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#torch.save(output_1L_model, 'fno2D_1l.h5')\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\azhir\\Desktop\\final_results\\fno2_attempt3.ipynb Cell 9\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, epochs, batch_size, learning_rate, weight_decay, step_size, gamma)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m#print('in: ',x.shape)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m out \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m#print('out: ',out.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m mse \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(out\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), y\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\azhir\\anaconda3\\envs\\gundam\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\azhir\\Desktop\\final_results\\fno2_attempt3.ipynb Cell 9\u001b[0m in \u001b[0;36mFNO2d_edit.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     grid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_grid(x\u001b[39m.\u001b[39;49mshape, x\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x, grid), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m#print('shape before concat', x.shape)\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\azhir\\Desktop\\final_results\\fno2_attempt3.ipynb Cell 9\u001b[0m in \u001b[0;36mFNO2d_edit.get_grid\u001b[1;34m(self, shape, device)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_grid\u001b[39m(\u001b[39mself\u001b[39m, shape, device):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     batchsize, size_x, size_y \u001b[39m=\u001b[39m shape[\u001b[39m0\u001b[39m], shape[\u001b[39m1\u001b[39m], shape[\u001b[39m2\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     gridx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39mlinspace(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, size_x), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/azhir/Desktop/final_results/fno2_attempt3.ipynb#X11sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     gridx \u001b[39m=\u001b[39m gridx\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, size_x, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mrepeat([batchsize, \u001b[39m1\u001b[39m, size_y, \u001b[39m1\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# SIGMA - TEST NETWORK\n",
    "############################################################################\n",
    "\n",
    "model_fcn_sigma = FNO2d_edit(modes, modes, width).cuda()\n",
    "print(count_params(model_fcn_sigma))\n",
    "output_sigma = train_model(model_fcn_sigma, 20, batch_size, learning_rate, weight_decay, step_size, gamma)\n",
    "\n",
    "output_sigma_model = output_sigma.get(\"model\")\n",
    "#torch.save(output_1L_model, 'fno2D_1l.h5')\n",
    "results_sigma = model_metrics(output_sigma_model,test_loader,ntrain)\n",
    "\n",
    "# TEST NETWORK RESULTS\n",
    "############################################################################\n",
    "K = torch.unsqueeze(test_loader.dataset[test_num][0], 0).cuda()\n",
    "output_sigma_model.eval()\n",
    "predicted = np.reshape(output_sigma_model(K).detach().cpu().numpy(),(16,16))\n",
    "plt.imshow(predicted)\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('gundam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cddc3d62f9a7e02ecb46a81ab11648d63c0573e5e13f25e036352996103f8463"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
