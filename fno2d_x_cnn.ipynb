{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\azhir\\anaconda3\\envs\\gundam\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# initial set-up\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "from timeit import default_timer\n",
    "from utilities3 import *\n",
    "\n",
    "from Adam import Adam\n",
    "from sewar.full_ref import rmse, uqi\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CONFIGURATIONS - HYPER PARAMETERS\n",
    "################################################################\n",
    "#  configurations - HYPER PARAMETERS\n",
    "################################################################\n",
    "\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 20\n",
    "step_size = 50\n",
    "gamma = 0.5\n",
    "\n",
    "modes = 5\n",
    "width = 64\n",
    "\n",
    "weight_decay=1e-4\n",
    "\n",
    "# Hyperparameters to change - epoch{10,100,200}, step_size{}, batchsize{}, gamma{}, modes{}, weight_decay{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: measurement torch.Size([8000, 128, 1]) solution torch.Size([8000, 16, 16, 1])\n",
      "test: measurement torch.Size([2000, 128, 1]) solution torch.Size([2000, 16, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "# CHANGED INPUTS\n",
    "# DATA LOADING\n",
    "############################## DATA LOADING ########################################\n",
    "raw_data = scipy.io.loadmat('./datasets/training_data_16x16_10k')\n",
    "\n",
    "sol = np.asarray(raw_data['solution_data'])\n",
    "mes = np.asarray(raw_data['measurement_data'])\n",
    "\n",
    "sol_temp = np.expand_dims(sol,axis = 1 )\n",
    "#sol_temp = np.sqeeze(sol_temp)\n",
    "mes_temp = np.transpose(np.expand_dims(mes,axis = 1 ),(0,2,1))\n",
    "\n",
    "# dimensions of solution space\n",
    "y_dim1 = 16\n",
    "y_dim2 = 16\n",
    "\n",
    "# dimensions of measurement space\n",
    "#x_dim2 = 8     # no of detectors\n",
    "#x_dim1 = int(mes.shape[1]/x_dim2)\n",
    "\n",
    "def convert_data(data_x, data_y):\n",
    "    data_X = torch.from_numpy(data_x).float()\n",
    "    data_Y = torch.from_numpy(data_y).float()\n",
    "    return data_X, data_Y\n",
    "\n",
    "# we are solving the inverse problem, so going from measurements, to solutions\n",
    "X, y = convert_data(mes_temp, sol_temp)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#y_normalizer = UnitGaussianNormalizer(y_train)\n",
    "#y_train = y_normalizer.encode(y_train)\n",
    "\n",
    "#x_normalizer = UnitGaussianNormalizer(X_train)\n",
    "#X_train = x_normalizer.encode(X_train)\n",
    "#X_test = x_normalizer.encode(X_test)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "y_test = y_test.reshape(y_test.shape[0],y_dim1,y_dim2,1)\n",
    "y_train = y_train.reshape(y_train.shape[0],y_dim1,y_dim2,1)\n",
    "\n",
    "\n",
    "#X_test = X_test.reshape(X_test.shape[0], x_dim1,x_dim2,1)\n",
    "#X_train = X_train.reshape(X_train.shape[0], x_dim1,x_dim2,1)\n",
    "\n",
    "#batch_size = 10\n",
    "#train_loader = DataLoader(data_utils.TensorDataset(X_train, Y_train), batch_size=batch_size, shuffle=True)\n",
    "train_dataset = TensorDataset( X_train, y_train )\n",
    "test_dataset = TensorDataset( X_test, y_test )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "ntrain = X_train.shape[0]\n",
    "ntest = X_test.shape[0]\n",
    "\n",
    "\n",
    "print('training: measurement', X_train.shape, 'solution', y_train.shape)\n",
    "print('test: measurement', X_test.shape, 'solution', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL METRICS\n",
    "###################################\n",
    "# MODEL METRICS\n",
    "###################################\n",
    "def model_metrics(model,test_loader,ntrain):\n",
    "    rms_values, psnr_values, ssim_values, uqi_values = [], [], [], []\n",
    "\n",
    "    for test_num in range(ntest):\n",
    "        K = torch.unsqueeze(test_loader.dataset[test_num][0], 0).cuda()\n",
    "        model.eval()\n",
    "        predicted_np = np.reshape(model(K).detach().cpu().numpy(),(16,16))\n",
    "\n",
    "        truth = test_loader.dataset[test_num][1]\n",
    "        truth_np = np.reshape(truth.numpy(),(16,16))\n",
    "\n",
    "        #diff_image = predicted - truth_np\n",
    "        #np.sqrt(np.sum(diff_image**2)/256)\n",
    "\n",
    "        rms_values.append(rmse(predicted_np, truth_np))\n",
    "        psnr_values.append(psnr(truth_np, predicted_np, data_range=predicted_np.max() - predicted_np.min()))\n",
    "        ssim_values.append(ssim(truth_np, predicted_np, data_range=predicted_np.max() - predicted_np.min()))\n",
    "        uqi_values.append(uqi(predicted_np, truth_np))\n",
    "\n",
    "    model_rms = sum(rms_values)/ len(rms_values)\n",
    "    std_rms = np.std(np.array(rms_values))\n",
    "\n",
    "    model_psnr = sum(psnr_values)/ len(psnr_values)\n",
    "    std_psnr = np.std(np.array(psnr_values))\n",
    "\n",
    "    model_ssim = sum(ssim_values)/ len(ssim_values)\n",
    "    std_ssim = np.std(np.array(ssim_values))\n",
    "\n",
    "    model_uqi = sum(uqi_values)/ len(uqi_values)\n",
    "    std_uqi = np.std(np.array(uqi_values))\n",
    "\n",
    "\n",
    "    print(\"RMSE: \", model_rms, std_rms, sep=\"---\")\n",
    "    print(\"PSNR: \", model_psnr, std_psnr, sep=\"---\")\n",
    "    print(\"SSIM: \", model_ssim, std_ssim, sep=\"---\")\n",
    "    print(\"UQI: \", model_uqi, std_uqi, sep=\"---\")\n",
    "\n",
    "\n",
    "    output = {\n",
    "    \"rms\": rms_values,\n",
    "    \"psnr\": psnr_values,\n",
    "    \"ssim\": ssim_values,\n",
    "    \"uqi\": uqi_values,\n",
    "    }\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   DEFINING THE TOP 2D FOURIER LAYER\n",
    "#####################################################################\n",
    "#   DEFINING THE TOP 2D FOURIER LAYER\n",
    "\n",
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        #print('input',input.shape)\n",
    "        #print('weights',weights.shape)\n",
    "        #print(\"bixy,ioxy->boxy\")\n",
    "        R_out = torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "        #print(R_out.shape)\n",
    "        return R_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        #print('what is out_ft', out_ft.shape)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNO2d(nn.Module):\n",
    "    def __init__(self, modes1, modes2,  width):\n",
    "        super(FNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        \n",
    "        #self.padding = 16 # pad the domain if input is non-periodic\n",
    "        #self.fc0 = nn.Linear(3, self.width) # input channel is 3: (a(x, y), x, y)\n",
    "        \n",
    "\n",
    "        #nn.Conv2d(in_channels = 1, out_channels = self.width, kernel_size = (3,3), stride=(3,3), padding=0)\n",
    "        self.mapping = nn.Linear(128, 256)      # 8x16=128 size of measurements  to 16x16 = 256 size of image \n",
    "\n",
    "        # CNN SECTION - not necessary \n",
    "        #self.cnn1 = nn.Conv2d(1, 128, kernel_size=5, stride=1, padding=2)\n",
    "        #self.cnn2 = nn.Conv2d(128, 1, kernel_size=1, stride=1, padding=0)\n",
    "        # CNN SECTION - not necessary \n",
    "\n",
    "\n",
    "        self.fc0 = nn.Linear(1, self.width)\n",
    "        \n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128,1) \n",
    "\n",
    "        #self.fc1 = nn.Linear(self.width, 128)\n",
    "        #self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.mapping(x[:,:,0])\n",
    "        x = x.reshape(x.shape[0],16,16,1)\n",
    "        \n",
    "        # CNN SECTION - not necessary \n",
    "        #x = x.permute(0, 3, 1, 2)\n",
    "        #x = self.cnn1(x)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        #x = self.cnn2(x)        \n",
    "        #x = x.permute(0, 2, 3, 1)\n",
    "        # CNN SECTION - not necessary \n",
    "\n",
    "\n",
    "\n",
    "        x = self.fc0(x)\n",
    "        #x = self.fc02(x)\n",
    "        #print('fc0 shape',x.shape)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        #x = F.pad(x, [0,self.padding, 0,self.padding])\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "        #print('FNO output shape', x.shape)\n",
    "        #x = x[..., :-self.padding, :-self.padding]\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND EVALUATION\n",
    "################################################################\n",
    "# TRAINING AND EVALUATION\n",
    "################################################################\n",
    "\n",
    "\n",
    "def train_model(model, epochs, batch_size, learning_rate, weight_decay, step_size, gamma):\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    train_losses, test_losses, epoch_time = [], [], []\n",
    "\n",
    "    myloss = LpLoss(size_average=False)\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        t1 = default_timer()\n",
    "        train_mse = 0\n",
    "        train_l2 = 0\n",
    "        total_time = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            #print('in: ',x.shape)\n",
    "            out = model(x)\n",
    "            #print('out: ',out.shape)\n",
    "\n",
    "            mse = F.mse_loss(out.view(batch_size, -1), y.view(batch_size, -1), reduction='mean')\n",
    "            l2 = myloss(out.view(batch_size, -1), y.view(batch_size, -1))\n",
    "            l2.backward() # use the l2 relative loss\n",
    "\n",
    "            optimizer.step()\n",
    "            train_mse += mse.item()\n",
    "            train_l2 += l2.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        model.eval()\n",
    "        test_l2 = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "\n",
    "                out = model(x)\n",
    "                test_l2 += myloss(out.view(batch_size, -1), y.view(batch_size, -1)).item()\n",
    "\n",
    "        #train_mse /= len(train_loader)\n",
    "        train_l2 /= ntrain\n",
    "        train_losses.append(train_l2)\n",
    "        test_l2 /= ntest\n",
    "        test_losses.append(test_l2)\n",
    "\n",
    "\n",
    "        t2 = default_timer()\n",
    "        epoch_time.append(t2 - t1)\n",
    "\n",
    "        print(ep, t2 - t1, train_l2, test_l2)\n",
    "    total_time = sum(epoch_time)\n",
    "    print(\"TOTAL TIME: \", total_time, sep=\"---\")\n",
    "\n",
    "    output = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"epoch_time\": epoch_time,\n",
    "    \"training_time\": total_time,\n",
    "    \"model\": model\n",
    "    }\n",
    "    return output \n",
    "# torch.save(model, 'model/ns_fourier_burgers')\n",
    "\n",
    "# scipy.io.savemat('pred/burger_test.mat', mdict={'pred': pred.cpu().numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1696641\n",
      "0 24.65441760001704 0.19291693280637265 0.1570418580174446\n",
      "1 12.169320399989374 0.15065596625208855 0.14348254352808\n",
      "2 11.313418200006709 0.14310735814273357 0.14948631197214127\n",
      "3 11.47757960006129 0.13867206949740649 0.12846920147538185\n",
      "4 11.388009099988267 0.13397905291616916 0.13138867315649985\n",
      "5 11.808552600094117 0.13187124012410642 0.13502318650484085\n",
      "6 11.432510699960403 0.13010642066597938 0.13308304738998414\n",
      "7 11.375525000039488 0.12811237975209952 0.12251849806308747\n",
      "8 11.427845900063403 0.12609388962388038 0.1302855976819992\n",
      "9 11.487210399936885 0.12452245574444533 0.1284248905479908\n",
      "10 11.443565299967304 0.12264689822494984 0.122573879301548\n",
      "11 11.515443199896254 0.12322356221079826 0.12848048332333564\n",
      "12 11.433547500055283 0.1212722643390298 0.12075415953993797\n",
      "13 11.55506299994886 0.12009299251437187 0.1205106911957264\n",
      "14 11.377311900025234 0.12063470968604088 0.12245976361632348\n",
      "15 11.455892800004221 0.11875707963109017 0.11921290257573128\n",
      "16 11.270669300109148 0.11899147680401802 0.11485924530029297\n",
      "17 11.536833900026977 0.11702229983359576 0.11746486279368401\n",
      "18 11.448400199995376 0.11654730669409037 0.1207511186003685\n",
      "19 11.923333600047044 0.11608209285885096 0.11991572099924087\n",
      "TOTAL TIME: ---243.49445020023268\n",
      "RMSE: ---0.0032226414671537003---0.0013793933421546358\n",
      "PSNR: ---23.900442308902328---3.0137625771982623\n",
      "SSIM: ---0.8442634589024179---0.061805586010693754\n",
      "UQI: ---0.9922956765424357---0.005374341459464972\n"
     ]
    }
   ],
   "source": [
    "# ONE LAYER RESULTS\n",
    "############################################################################\n",
    "\n",
    "model_1L = FNO2d(modes, modes, width).cuda()\n",
    "print(count_params(model_1L))\n",
    "output_1L = train_model(model_1L, epochs, batch_size, learning_rate, weight_decay, step_size, gamma)\n",
    "\n",
    "output_1L_model = output_1L.get(\"model\")\n",
    "#torch.save(output_1L_model, 'fno2D_1l.h5')\n",
    "results_1L = model_metrics(output_1L_model,test_loader,ntrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e623798610>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_num = 434\n",
    "K = torch.unsqueeze(test_loader.dataset[test_num][0], 0).cuda()\n",
    "output_1L_model.eval()\n",
    "predicted = np.reshape(output_1L_model(K).detach().cpu().numpy(),(16,16))\n",
    "plt.imshow(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(output_1L_model, 'fno2D_fcn_input.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1e6a0b77220>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Actual\n",
    "#test_num = 434\n",
    "truth_out = test_loader.dataset[test_num][1]\n",
    "plt.imshow(np.reshape(truth_out.numpy(),(16,16)))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n#torch.save(output_1L_model, 'fno2D_cnn.h5')\\n1700098\\nTOTAL TIME: ---258.8476620001602\\nRMSE: ---0.003517735468301389---0.0014018399800621013\\nPSNR: ---22.889248447163705---2.6361724844055194\\nSSIM: ---0.8183652595353574---0.06562534731671044\\nUQI: ---0.99086240901261---0.006032708030571362\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4 Layer - editted FNO\n",
    "'''\n",
    "#torch.save(output_1L_model, 'fno2D_fcn_input.h5')\n",
    "1696641\n",
    "TOTAL TIME: ---243.49445020023268\n",
    "RMSE: ---0.0032226414671537003---0.0013793933421546358\n",
    "PSNR: ---23.900442308902328---3.0137625771982623\n",
    "SSIM: ---0.8442634589024179---0.061805586010693754\n",
    "UQI: ---0.9922956765424357---0.005374341459464972\n",
    "'''\n",
    "\n",
    "# 4 Layers - Combined with CNN\n",
    "''' \n",
    "#torch.save(output_1L_model, 'fno2D_cnn.h5')\n",
    "1700098\n",
    "TOTAL TIME: ---258.8476620001602\n",
    "RMSE: ---0.003517735468301389---0.0014018399800621013\n",
    "PSNR: ---22.889248447163705---2.6361724844055194\n",
    "SSIM: ---0.8183652595353574---0.06562534731671044\n",
    "UQI: ---0.99086240901261---0.006032708030571362\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "cddc3d62f9a7e02ecb46a81ab11648d63c0573e5e13f25e036352996103f8463"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
